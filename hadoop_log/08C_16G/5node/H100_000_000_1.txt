17/06/28 14:36:01 INFO common.AbstractJob: Command line arguments: {--alpha=[40], --endPhase=[2147483647], --implicitFeedback=[false], --input=[hdfs://sd-spark01.localdomain:9000/data/output_data_naist_tsv/100000000.tsv], --lambda=[1000], --numFeatures=[100000000], --numIterations=[30000000], --numThreadsPerSolver=[1], --output=[hdfs://sd-spark01.localdomain:9000/H100_000_000], --startPhase=[0], --tempDir=[hdfs://sd-spark01.localdomain:9000/tmp1]}
17/06/28 14:36:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/06/28 14:36:01 INFO Configuration.deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir
17/06/28 14:36:01 INFO Configuration.deprecation: mapred.compress.map.output is deprecated. Instead, use mapreduce.map.output.compress
17/06/28 14:36:01 INFO Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
17/06/28 14:36:01 INFO client.RMProxy: Connecting to ResourceManager at sd-spark01.localdomain/163.221.29.42:8032
17/06/28 14:36:02 INFO input.FileInputFormat: Total input paths to process : 1
17/06/28 14:36:02 INFO mapreduce.JobSubmitter: number of splits:12
17/06/28 14:36:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1498286170644_0118
17/06/28 14:36:03 INFO impl.YarnClientImpl: Submitted application application_1498286170644_0118
17/06/28 14:36:03 INFO mapreduce.Job: The url to track the job: http://sd-spark01.localdomain:8088/proxy/application_1498286170644_0118/
17/06/28 14:36:03 INFO mapreduce.Job: Running job: job_1498286170644_0118
17/06/28 14:36:07 INFO mapreduce.Job: Job job_1498286170644_0118 running in uber mode : false
17/06/28 14:36:07 INFO mapreduce.Job:  map 0% reduce 0%
17/06/28 14:36:17 INFO mapreduce.Job:  map 4% reduce 0%
17/06/28 14:36:18 INFO mapreduce.Job:  map 18% reduce 0%
17/06/28 14:36:19 INFO mapreduce.Job:  map 29% reduce 0%
17/06/28 14:36:22 INFO mapreduce.Job:  map 30% reduce 0%
17/06/28 14:36:23 INFO mapreduce.Job:  map 32% reduce 0%
17/06/28 14:36:24 INFO mapreduce.Job:  map 36% reduce 0%
17/06/28 14:36:25 INFO mapreduce.Job:  map 39% reduce 0%
17/06/28 14:36:27 INFO mapreduce.Job:  map 40% reduce 0%
17/06/28 14:36:28 INFO mapreduce.Job:  map 46% reduce 3%
17/06/28 14:36:29 INFO mapreduce.Job:  map 48% reduce 3%
17/06/28 14:36:30 INFO mapreduce.Job:  map 49% reduce 3%
17/06/28 14:36:32 INFO mapreduce.Job:  map 50% reduce 3%
17/06/28 14:36:33 INFO mapreduce.Job:  map 54% reduce 3%
17/06/28 14:36:34 INFO mapreduce.Job:  map 57% reduce 3%
17/06/28 14:36:35 INFO mapreduce.Job:  map 58% reduce 3%
17/06/28 14:36:37 INFO mapreduce.Job:  map 63% reduce 3%
17/06/28 14:36:38 INFO mapreduce.Job:  map 64% reduce 3%
17/06/28 14:36:39 INFO mapreduce.Job:  map 66% reduce 3%
17/06/28 14:36:41 INFO mapreduce.Job:  map 71% reduce 3%
17/06/28 14:36:42 INFO mapreduce.Job:  map 74% reduce 3%
17/06/28 14:36:43 INFO mapreduce.Job:  map 77% reduce 3%
17/06/28 14:36:44 INFO mapreduce.Job:  map 78% reduce 3%
17/06/28 14:36:45 INFO mapreduce.Job:  map 83% reduce 3%
17/06/28 14:36:46 INFO mapreduce.Job:  map 84% reduce 6%
17/06/28 14:36:49 INFO mapreduce.Job:  map 100% reduce 17%
17/06/28 14:36:55 INFO mapreduce.Job:  map 100% reduce 37%
17/06/28 14:36:58 INFO mapreduce.Job:  map 100% reduce 49%
17/06/28 14:37:01 INFO mapreduce.Job:  map 100% reduce 61%
17/06/28 14:37:04 INFO mapreduce.Job:  map 100% reduce 68%
17/06/28 14:37:07 INFO mapreduce.Job:  map 100% reduce 70%
17/06/28 14:37:10 INFO mapreduce.Job:  map 100% reduce 73%
17/06/28 14:37:13 INFO mapreduce.Job:  map 100% reduce 76%
17/06/28 14:37:16 INFO mapreduce.Job:  map 100% reduce 79%
17/06/28 14:37:19 INFO mapreduce.Job:  map 100% reduce 81%
17/06/28 14:37:22 INFO mapreduce.Job:  map 100% reduce 84%
17/06/28 14:37:25 INFO mapreduce.Job:  map 100% reduce 87%
17/06/28 14:37:28 INFO mapreduce.Job:  map 100% reduce 90%
17/06/28 14:37:31 INFO mapreduce.Job:  map 100% reduce 92%
17/06/28 14:37:34 INFO mapreduce.Job:  map 100% reduce 95%
17/06/28 14:37:37 INFO mapreduce.Job:  map 100% reduce 98%
17/06/28 14:37:40 INFO mapreduce.Job:  map 100% reduce 100%
17/06/28 14:37:45 INFO mapreduce.Job: Job job_1498286170644_0118 completed successfully
17/06/28 14:37:45 INFO mapreduce.Job: Counters: 51
	File System Counters
		FILE: Number of bytes read=886392032
		FILE: Number of bytes written=1294392044
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1498104215
		HDFS: Number of bytes written=929561588
		HDFS: Number of read operations=39
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Killed map tasks=2
		Launched map tasks=14
		Launched reduce tasks=1
		Data-local map tasks=11
		Rack-local map tasks=3
		Total time spent by all maps in occupied slots (ms)=500460
		Total time spent by all reduces in occupied slots (ms)=84544
		Total time spent by all map tasks (ms)=500460
		Total time spent by all reduce tasks (ms)=84544
		Total vcore-milliseconds taken by all map tasks=500460
		Total vcore-milliseconds taken by all reduce tasks=84544
		Total megabyte-milliseconds taken by all map tasks=512471040
		Total megabyte-milliseconds taken by all reduce tasks=86573056
	Map-Reduce Framework
		Map input records=100000000
		Map output records=100000000
		Map output bytes=1820167821
		Map output materialized bytes=406583585
		Input split bytes=1680
		Combine input records=100764598
		Combine output records=976939
		Reduce input groups=17770
		Reduce shuffle bytes=406583585
		Reduce input records=212341
		Reduce output records=17770
		Spilled Records=1224806
		Shuffled Maps =12
		Failed Shuffles=0
		Merged Map outputs=12
		GC time elapsed (ms)=18353
		CPU time spent (ms)=556440
		Physical memory (bytes) snapshot=3770093568
		Virtual memory (bytes) snapshot=27817971712
		Total committed heap usage (bytes)=2684878848
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=1498102535
	File Output Format Counters 
		Bytes Written=929561588
17/06/28 14:37:45 INFO client.RMProxy: Connecting to ResourceManager at sd-spark01.localdomain/163.221.29.42:8032
17/06/28 14:37:45 INFO input.FileInputFormat: Total input paths to process : 1
17/06/28 14:37:46 INFO mapreduce.JobSubmitter: number of splits:7
17/06/28 14:37:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1498286170644_0119
17/06/28 14:37:46 INFO impl.YarnClientImpl: Submitted application application_1498286170644_0119
17/06/28 14:37:46 INFO mapreduce.Job: The url to track the job: http://sd-spark01.localdomain:8088/proxy/application_1498286170644_0119/
17/06/28 14:37:46 INFO mapreduce.Job: Running job: job_1498286170644_0119
17/06/28 14:37:51 INFO mapreduce.Job: Job job_1498286170644_0119 running in uber mode : false
17/06/28 14:37:51 INFO mapreduce.Job:  map 0% reduce 0%
17/06/28 14:38:00 INFO mapreduce.Job:  map 2% reduce 0%
17/06/28 14:38:02 INFO mapreduce.Job:  map 13% reduce 0%
17/06/28 14:38:03 INFO mapreduce.Job:  map 15% reduce 0%
17/06/28 14:38:05 INFO mapreduce.Job:  map 17% reduce 0%
17/06/28 14:38:08 INFO mapreduce.Job:  map 24% reduce 0%
17/06/28 14:38:09 INFO mapreduce.Job:  map 26% reduce 0%
17/06/28 14:38:14 INFO mapreduce.Job:  map 35% reduce 0%
17/06/28 14:38:15 INFO mapreduce.Job:  map 36% reduce 0%
17/06/28 14:38:20 INFO mapreduce.Job:  map 43% reduce 0%
17/06/28 14:38:21 INFO mapreduce.Job:  map 44% reduce 0%
17/06/28 14:38:23 INFO mapreduce.Job:  map 47% reduce 0%
17/06/28 14:38:27 INFO mapreduce.Job:  map 48% reduce 0%
17/06/28 14:38:29 INFO mapreduce.Job:  map 57% reduce 0%
17/06/28 14:38:30 INFO mapreduce.Job:  map 58% reduce 0%
17/06/28 14:38:33 INFO mapreduce.Job:  map 62% reduce 0%
17/06/28 14:38:35 INFO mapreduce.Job:  map 70% reduce 0%
17/06/28 14:38:38 INFO mapreduce.Job:  map 71% reduce 0%
17/06/28 14:38:44 INFO mapreduce.Job:  map 83% reduce 0%
17/06/28 14:38:47 INFO mapreduce.Job:  map 100% reduce 0%
17/06/28 14:38:51 INFO mapreduce.Job:  map 100% reduce 5%
17/06/28 14:38:57 INFO mapreduce.Job:  map 100% reduce 24%
17/06/28 14:39:00 INFO mapreduce.Job:  map 100% reduce 69%
17/06/28 14:39:03 INFO mapreduce.Job:  map 100% reduce 72%
17/06/28 14:39:06 INFO mapreduce.Job:  map 100% reduce 75%
17/06/28 14:39:09 INFO mapreduce.Job:  map 100% reduce 77%
17/06/28 14:39:12 INFO mapreduce.Job:  map 100% reduce 80%
17/06/28 14:39:15 INFO mapreduce.Job:  map 100% reduce 83%
17/06/28 14:39:18 INFO mapreduce.Job:  map 100% reduce 86%
17/06/28 14:39:21 INFO mapreduce.Job:  map 100% reduce 89%
17/06/28 14:39:24 INFO mapreduce.Job:  map 100% reduce 91%
17/06/28 14:39:27 INFO mapreduce.Job:  map 100% reduce 94%
17/06/28 14:39:30 INFO mapreduce.Job:  map 100% reduce 97%
17/06/28 14:39:33 INFO mapreduce.Job:  map 100% reduce 100%
17/06/28 14:39:34 INFO mapreduce.Job: Job job_1498286170644_0119 completed successfully
17/06/28 14:39:34 INFO mapreduce.Job: Counters: 50
	File System Counters
		FILE: Number of bytes read=539084262
		FILE: Number of bytes written=796351546
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=931005623
		HDFS: Number of bytes written=928138083
		HDFS: Number of read operations=31
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=7
		Launched reduce tasks=1
		Data-local map tasks=7
		Total time spent by all maps in occupied slots (ms)=428565
		Total time spent by all reduces in occupied slots (ms)=50278
		Total time spent by all map tasks (ms)=428565
		Total time spent by all reduce tasks (ms)=50278
		Total vcore-milliseconds taken by all map tasks=428565
		Total vcore-milliseconds taken by all reduce tasks=50278
		Total megabyte-milliseconds taken by all map tasks=438850560
		Total megabyte-milliseconds taken by all reduce tasks=51484672
	Map-Reduce Framework
		Map input records=17770
		Map output records=100000000
		Map output bytes=2107594908
		Map output materialized bytes=256397563
		Input split bytes=903
		Combine input records=117291793
		Combine output records=20554316
		Reduce input groups=480182
		Reduce shuffle bytes=256397563
		Reduce input records=3262523
		Reduce output records=480182
		Spilled Records=23816839
		Shuffled Maps =7
		Failed Shuffles=0
		Merged Map outputs=7
		GC time elapsed (ms)=15798
		CPU time spent (ms)=490650
		Physical memory (bytes) snapshot=2322800640
		Virtual memory (bytes) snapshot=17102766080
		Total committed heap usage (bytes)=1621098496
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=931004720
	File Output Format Counters 
		Bytes Written=928138083
	org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJob$Stats
		NUM_USERS=480182
17/06/28 14:39:34 INFO client.RMProxy: Connecting to ResourceManager at sd-spark01.localdomain/163.221.29.42:8032
17/06/28 14:39:34 INFO input.FileInputFormat: Total input paths to process : 1
17/06/28 14:39:34 INFO mapreduce.JobSubmitter: number of splits:7
17/06/28 14:39:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1498286170644_0120
17/06/28 14:39:34 INFO impl.YarnClientImpl: Submitted application application_1498286170644_0120
17/06/28 14:39:34 INFO mapreduce.Job: The url to track the job: http://sd-spark01.localdomain:8088/proxy/application_1498286170644_0120/
17/06/28 14:39:34 INFO mapreduce.Job: Running job: job_1498286170644_0120
17/06/28 14:39:39 INFO mapreduce.Job: Job job_1498286170644_0120 running in uber mode : false
17/06/28 14:39:39 INFO mapreduce.Job:  map 0% reduce 0%
17/06/28 14:39:45 INFO mapreduce.Job:  map 14% reduce 0%
17/06/28 14:39:47 INFO mapreduce.Job:  map 100% reduce 0%
17/06/28 14:39:50 INFO mapreduce.Job:  map 100% reduce 100%
17/06/28 14:39:51 INFO mapreduce.Job: Job job_1498286170644_0120 completed successfully
17/06/28 14:39:51 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=156350
		FILE: Number of bytes written=1185520
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=931005623
		HDFS: Number of bytes written=160048
		HDFS: Number of read operations=31
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=7
		Launched reduce tasks=1
		Data-local map tasks=7
		Total time spent by all maps in occupied slots (ms)=34746
		Total time spent by all reduces in occupied slots (ms)=2911
		Total time spent by all map tasks (ms)=34746
		Total time spent by all reduce tasks (ms)=2911
		Total vcore-milliseconds taken by all map tasks=34746
		Total vcore-milliseconds taken by all reduce tasks=2911
		Total megabyte-milliseconds taken by all map tasks=35579904
		Total megabyte-milliseconds taken by all reduce tasks=2980864
	Map-Reduce Framework
		Map input records=17770
		Map output records=17770
		Map output bytes=374430
		Map output materialized bytes=159457
		Input split bytes=903
		Combine input records=17770
		Combine output records=7
		Reduce input groups=1
		Reduce shuffle bytes=159457
		Reduce input records=7
		Reduce output records=1
		Spilled Records=14
		Shuffled Maps =7
		Failed Shuffles=0
		Merged Map outputs=7
		GC time elapsed (ms)=1130
		CPU time spent (ms)=17740
		Physical memory (bytes) snapshot=2136129536
		Virtual memory (bytes) snapshot=17066450944
		Total committed heap usage (bytes)=1523056640
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=931004720
	File Output Format Counters 
		Bytes Written=160048
17/06/28 14:39:52 INFO ipc.Client: Retrying connect to server: sd-spark06.localdomain/163.221.29.47:34696. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
17/06/28 14:39:53 INFO ipc.Client: Retrying connect to server: sd-spark06.localdomain/163.221.29.47:34696. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
17/06/28 14:39:54 INFO ipc.Client: Retrying connect to server: sd-spark06.localdomain/163.221.29.47:34696. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)
17/06/28 14:39:54 INFO mapred.ClientServiceDelegate: Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server
17/06/28 14:39:55 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:39:56 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:39:57 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:39:58 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:39:59 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:00 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:01 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:02 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:03 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:04 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:04 INFO mapred.ClientServiceDelegate: Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server
17/06/28 14:40:05 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:06 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:07 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:08 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:09 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:10 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:11 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:12 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:13 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:14 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:14 INFO mapred.ClientServiceDelegate: Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server
17/06/28 14:40:15 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:16 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:17 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:18 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:19 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:20 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:21 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:22 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:23 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
17/06/28 14:40:24 INFO ipc.Client: Retrying connect to server: sd-spark01.localdomain/163.221.29.42:10020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
Exception in thread "main" java.io.IOException: java.net.ConnectException: Call From sd-spark01.localdomain/163.221.29.42 to sd-spark01.localdomain:10020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:338)
	at org.apache.hadoop.mapred.ClientServiceDelegate.getJobCounters(ClientServiceDelegate.java:375)
	at org.apache.hadoop.mapred.YARNRunner.getJobCounters(YARNRunner.java:565)
	at org.apache.hadoop.mapreduce.Job$7.run(Job.java:768)
	at org.apache.hadoop.mapreduce.Job$7.run(Job.java:765)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)
	at org.apache.hadoop.mapreduce.Job.getCounters(Job.java:765)
	at org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJob.run(ParallelALSFactorizationJob.java:195)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
	at org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJob.main(ParallelALSFactorizationJob.java:108)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.ConnectException: Call From sd-spark01.localdomain/163.221.29.42 to sd-spark01.localdomain:10020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1474)
	at org.apache.hadoop.ipc.Client.call(Client.java:1401)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy14.getCounters(Unknown Source)
	at org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getCounters(MRClientProtocolPBClientImpl.java:166)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:324)
	... 18 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1523)
	at org.apache.hadoop.ipc.Client.call(Client.java:1440)
	... 27 more

real	4m29.251s
user	0m12.939s
sys	0m3.765s
